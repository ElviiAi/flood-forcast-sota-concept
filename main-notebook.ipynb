{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Project: ICEYE test on flood - Draft Plan\n",
    "\n",
    "## 1. Data Acquisition and Preprocessing\n",
    "\n",
    "### Chosen Region: Kuantan, Pahang, - Malaysia\n",
    "\n",
    "**Justification:**\n",
    "- **High Historical Flood Frequency:** Frequently affected by severe floods, particularly during the northeast monsoon season.\n",
    "- **Moderate Disaster Insurance Gap:** Many residents, of both poor and middle-class background - giving reason to approach this region to help the poor and vulnerable while  navigating the constraints of the capitalist system.\n",
    "- **High Commercial Potential for Flood-Extent Products:** Valuable for insurance companies, government agencies, and disaster relief organizations to improve resource allocation and emergency response strategies.\n",
    "- **High Potential for Flood-Extent Products:** Flood-extent products are a key component of flood risk management, and are used by insurance companies, government agencies, and disaster relief organizations to improve resource allocation and emergency response strategies.\n",
    "\n",
    "**Kuantan, Pahang:**\n",
    "Kuantan district, along with Temerloh and Pekan, is highly vulnerable to floods in Pahang state. [Source](https://www.oecd-ilibrary.org/sites/2621c072-en/index.html?itemId=%2Fcontent%2Fcomponent%2F2621c072-en)\n",
    "Kuantan faces significant flood risks due to its natural setting near the Kuantan River basin, changing climate, and development pressures. [Source](https://www.oecd-ilibrary.org/sites/2621c072-en/index.html?itemId=%2Fcontent%2Fcomponent%2F2621c072-en)\n",
    "\n",
    "\n",
    "\n",
    "### B. Data Sources\n",
    "- **DTM Data:**\n",
    "  - **Source:** Copernicus Open Access Hub (Sentinel).\n",
    "  - **Resolution:** High-resolution DTMs, processed to 10 meters.\n",
    "  - **Availability:** Freely accessible, comprehensive data.\n",
    "    - [Source](https://scihub.copernicus.eu/)\n",
    "- **SAR Data:**\n",
    "  - **Source:** Sentinel-1 via Copernicus Open Access Hub.\n",
    "  - **Frequency:** High-resolution C-band SAR data with a revisit time of 6 to 12 days.\n",
    "    - [Source](https://scihub.copernicus.eu/)\n",
    "- **Meteorological Data:**\n",
    "  - **Source:** METMalaysia and data.gov.my for weather patterns and conditions.\n",
    "  - **Availability:** Accessible through APIs and relevant meteorological services.\n",
    "    - [Source](https://met.gov.my/)\n",
    "    - [Source](https://data.gov.my/)\n",
    "\n",
    "### C. Preprocessing Steps\n",
    "1. **Download Data:**\n",
    "   - Obtain 30-meter resolution DTM data \n",
    "   - Acquire Sentinel-1 SAR data and meteorological data and resample to 30 meters.\n",
    "2. **Resampling and Normalization:**\n",
    "   - Use down sampling techniques to resample SAR data to 30 meters. (I'm not yet completely sure what effect this will have on the result - going with SAR resolution as the \"lowest common denominator\" might prove to be the optimal case\n",
    "   - Normalize SAR and meteorological data for consistency.\n",
    "3. **Handling Missing Data:**\n",
    "   - Apply interpolation or other methods to fill gaps in the datasets. If certain modalities are missing mith a frequency of 10-50% we can apply modality-wise drop out - making the model more robust to regions with limited multi-modal data,\n",
    "4. **Data Conversion:**\n",
    "   - Convert resampled and normalized data into the desired format (e.g., GeoTIFF (for later vis) / hdf5 or .pt [for data loading)).\n",
    "  - Quantise the data if necessary, depending on fine-tune/training computational req. and speed bfloat16 likely to be ideal post norm.\n",
    "\n",
    "## 2. Feature Engineering and Modeling\n",
    "\n",
    "### A. Feature Engineering\n",
    "- **Relationship Analysis:**\n",
    "  - Examine the connections between historical flood locations, Sentinel-1 revisit times, flood characteristics (e.g., size, duration), terrain data, and weather patterns.\n",
    "- **Feature Development:**\n",
    "  - Develop features predicting the probability of capturing a flood event within 24 hours.\n",
    "- **Feature Representation:**\n",
    "  - Given that our chosen model finds relationships within the time slice of the data and between them, we can represent each datapoint as a \"time node\" with DTM/SAR/Met features rolled out into a flat matrix. Such that the input to the model is a matrix of size (N<sub>k</sub>,T<sub>hr</sub>) where N<sub>1</sub> could be DTM/SAR/SAR<sub>sigma</sub>/SAR<sub>NDWI</sub> pixels or a certain time-series signal for the given area.\n",
    "\n",
    "### B. Modeling Approach\n",
    "- **Model Selection:**\n",
    "  - Utilize the SoTA PatchTSMixer model, configured for multivariate time series forecasting. [Source](https://huggingface.co/ibm-granite/granite-timeseries-patchtsmixer)\n",
    "    - ![PatchTSMixer](resources/img.png)\n",
    "        - \n",
    "    - Reason for selection: One can split time series forcasting into univariate (i.e. single signal) and multivariate (i.e. multiple signals) problems. The PatchTSMixer model is a multivariate model that can handle both. Not only that, it exceeds the previous state of the art by 8 - 60% in Mean Squared Error (MSE) which is quite massive.\n",
    "    - The design of the architecture is such that it does not care what physical phenomena or spatial distance your signal is away from the target. It is a \"patch\" model that can be applied to a combination of many different modalitites (unrolled imagerys akeen to DETR [one of the first transformer architectures to use transformers] for SAR, DTM for DTM, etc.) and can be trained on a variety of datasets.\n",
    "    - Ideally the architecture should be slightly modified to be more akeen to auto-regressive models. Such that the input to the model can be coninous signals of various modalities at every time step, and the output is a single continuous signal. At present, the model is trained on a 1-hour time-step *with a 21-day context window* (i.e. 21 hours of data before now can be fed into it). We also had to adopt to 7 features to represent the 7 modalities in the pre-trained model (I was hoping to get this running but only had less than a day and ran out of time) - in reality we can incorporate far more features at every time step, depending on the region size and models size applicability.\n",
    "    - THe additional benefit of using transformers is the insane acceleration of fine-tuning and transfer learning techniques; as an example we can borrow quantised  2,4,5,6,8 bit CUDA kernels from llama.cpp and apply them to the model.\n",
    "    - With techniques such at DPO and QLoRA fine-tuning, the model can constantly evolve without encountering catastrophic forgetting.\n",
    "\n",
    "- **Training Data:**\n",
    "  - Generate sequences of tokens representing 1-hour (or less) integrals of data from all modalities.\n",
    "  - Apply dropout in the input token layer to enhance robustness.\n",
    "    - \n",
    "- **Evaluation:**\n",
    "  - Assess model performance using metrics like AUC-ROC 2x2 conf mat, and F1-score (which is just binary accuracy in this case).\n",
    "  - Given time to modify the architecture into an autoregressive one, we can leverage each floor even instance (be it 3 hours or 24 hours) to train the model on a 21-day window of data to predict the next token of probability of a flood, and let it run for N time steps to try to make projections into the future. This would be akin to a \"forecast\" of the flood probability for the next 21 days that can take into account any modality no matter how physically or spatially distant it is from the others.\n",
    "  - Au auto-regressive architecture could also let you determine the accuracy of going into a new area. As we can see if feeding in the 21-day window of data from the new area, can predict the probability of flooding based on historical data.\n",
    "  - The strength of each signal inut and its predictive ability for the model can also we further evaluated by looking at the importance of each feature in the model, by levraging techniques like PCA.\n",
    "  - Model Calibration: By calibrating the model probability at each time step (after the feature-wise softmax) we can get a more accurate probability of flooding, rather than just a number between 0 and 1 which has no resemblance to the actual probability of flooding (always important to explain this factoid to clients)"
   ],
   "id": "ce56ea5d3b1e1c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the geopolygon of the area in question, calculate total area in sqr km, output to KML to sanity check with google earth",
   "id": "4e0c9fc18a65b907"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T07:39:52.610759Z",
     "start_time": "2024-05-15T07:39:52.207360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import geojson\n",
    "import shapely\n",
    "import simplekml\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, shape\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "import math\n",
    "from shapely.ops import transform\n",
    "\n",
    "def calculate_area_using_utm(input_geometry):\n",
    "    \"\"\"\n",
    "    Calculate the area of the input geometry in square meters using UTM projection.\n",
    "    transformation from WGS84 to UTM.\n",
    "    \"\"\"\n",
    "    from_epsg = 4326\n",
    "    to_epsg = get_utm_epsg(shapely.Point(input_geometry.centroid))\n",
    "\n",
    "    transformation = pyproj.Transformer.from_crs(\n",
    "        crs_from=pyproj.CRS.from_epsg(from_epsg),\n",
    "        crs_to=pyproj.CRS.from_epsg(to_epsg),\n",
    "        always_xy=True)\n",
    "\n",
    "    project = lambda x, y: transformation.transform(x, y)\n",
    "    transformed_geom = transform(project, input_geometry)\n",
    "\n",
    "    return transformed_geom.area\n",
    "\n",
    "def get_utm_epsg(point):\n",
    "    \"\"\"\n",
    "    Get the EPSG code for the UTM zone corresponding to a given WGS84 point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.Point): The point for which to determine the UTM zone.\n",
    "\n",
    "    Returns:\n",
    "        int: The EPSG code for the UTM zone.\n",
    "    \"\"\"\n",
    "    # Get the latitude and longitude of the point\n",
    "    lon = float(point.x)\n",
    "    lat = float(point.y)\n",
    "    zone_number = int(math.floor((lon + 180) / 6) + 1)\n",
    "    \n",
    "    # More accurate EPSG code for the Kuantan region (took a while to remember this xD)\n",
    "    if (0 <= lat < 7) or (14 <= lat < 21):\n",
    "        epsg_code = '327' + str(zone_number)\n",
    "    else:\n",
    "        epsg_code = '326' + str(zone_number)\n",
    "        \n",
    "    return epsg_code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the region name and coordinates\n",
    "region_name = \"Kuantan\"\n",
    "\n",
    "# Note polygon chosen using map-box - if proven successful, we can extend the polygon to include more area and determine generalization\n",
    "poc_extent = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"geometry\": {\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [103.30258965042253, 3.8536955341197654],\n",
    "            [103.30258965042253, 3.761091245587309],\n",
    "            [103.39347050856702, 3.761091245587309],\n",
    "            [103.39347050856702, 3.8536955341197654],\n",
    "            [103.30258965042253, 3.8536955341197654],\n",
    "            [103.30258965042253, 3.8536955341197654]\n",
    "          ]\n",
    "        ],\n",
    "        \"type\": \"Polygon\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a Feature object\n",
    "kuantan = shapely.geometry.Polygon.from_bounds(poc_extent['features'][0]['geometry']['coordinates'][0][0][0], poc_extent['features'][0]['geometry']['coordinates'][0][0][1], poc_extent['features'][0]['geometry']['coordinates'][0][2][0], poc_extent['features'][0]['geometry']['coordinates'][0][2][1])\n",
    "\n",
    "# Sanity check rea so our NN is not massive\n",
    "kuantan_area = calculate_area_using_utm(kuantan)\n",
    "print(kuantan_area) # 114_860 30x30m tiles"
   ],
   "id": "afc849b17a54dc9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103373524.18945646\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2, ## Modalities and Their Importance for Flood Prediction\n",
    "\n",
    "**Sentinel-1 Satellite Data (Radar Imagery):**\n",
    "\n",
    "* **Importance:** Radar imagery from the Sentinel-1 satellite is crucial for detecting changes in land surface and water bodies. It provides information about soil moisture, surface roughness, and standing water, which are essential indicators of flood conditions.\n",
    "* **Processing:** The raw amplitude data (VV and VH polarizations) are normalized to ensure consistent scaling and comparability across images and time points.\n",
    "\n",
    "**Copernicus DEM (Digital Elevation Model):**\n",
    "\n",
    "* **Importance:** Elevation and slope data from the Copernicus DEM offer critical insights into the topography of the area. This information is essential for understanding water flow patterns and identifying areas prone to flooding.\n",
    "* **Processing:**  Elevation and slope data are normalized for consistency with other modalities and to aid the PatchTSMixer model's learning process.\n",
    "\n",
    "**METMalaysia Weather Data:**\n",
    "\n",
    "* **Importance:** Meteorological factors, particularly precipitation and temperature, play a significant role in flood events. High rainfall and elevated temperatures can lead to increased runoff and saturated soil conditions, increasing flood risk.\n",
    "* **Processing:** Precipitation and temperature data are normalized to align with model input requirements and ensure balanced feature importance.\n",
    "\n",
    "**Why This Specific Preprocessing?**\n",
    "\n",
    "The normalization step serves several crucial purposes:\n",
    "\n",
    "* **Comparability:** Allows comparison of data from diverse sources and time points on a common scale.\n",
    "* **Model Performance:** Enhances model convergence and stability, leading to faster training and improved performance.\n",
    "* **Feature Importance:** Prevents features with larger scales from dominating, ensuring all features contribute meaningfully to the prediction.\n",
    "\n",
    "**Additional Considerations for PatchTSMixer:**\n",
    "\n",
    "* **Channel Mapping:** Careful mapping of collected data to the seven channels expected by the PatchTSMixer model is required.\n",
    "* **Missing Data Handling:** The model's robustness to missing data is beneficial, but strategies for imputation or interpolation may be necessary if significant gaps exist in the collected data.\n"
   ],
   "id": "b78833a8f500ecda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T07:43:18.728858Z",
     "start_time": "2024-05-15T07:43:18.726106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.data_collector import FloodDataCollector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = datetime(2024, 1, 31)\n",
    "resolution = timedelta(hours=1)\n",
    "collector = FloodDataCollector(kuantan, start_date, end_date, resolution,output_path=\"train_val.pt\")\n",
    "collector.collect_data()"
   ],
   "id": "8cf505bd6d95e5a5",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1359084572.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[2], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    import FloodDataCollector(geojson_path, start_date, end_date, resolution)\u001B[0m\n\u001B[0m                             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3c043fc0f45e86a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. Determining the best time chunks for the data (for future development):\n",
    "\n",
    "\n",
    "Here are the key points regarding the frequency of SAR, DTM, and MET data in Malaysia:\n",
    "\n",
    "Sentinel-1 SAR:\n",
    "- The Sentinel-1 constellation has a repeat frequency of 6 days with two satellites, and 12 days with one satellite over Europe.\n",
    "- The Sentinel-1 C-SAR instrument has a centre frequency of 5.405 GHz.\n",
    "\n",
    "MET Malaysia weather data:\n",
    "- MET Malaysia provides daily updates for 7-day general forecast data.\n",
    "- Weather warning data, including earthquake data, is updated when required by MET Malaysia.\n",
    "- MET Malaysia has a network of automatic weather stations that provide real-time data, which is then processed at the headquarters to generate forecasts and warnings.\n",
    "- Historical rainfall data from meteorological stations operated by MET Malaysia is used in various research studies.\n",
    "\n",
    "Considering the Sentinel-1 SAR has a repeat frequency of 6-12 days, and MET Malaysia provides daily weather updates, I would use a context length that covers at least 12 days of historical data. This would ensure the model can capture the temporal patterns and variability in both the SAR imagery and corresponding weather conditions.\n",
    "\n",
    "For example, if you have hourly weather data, a 12-day context length would correspond to 288 hours (12 days * 24 hours/day). If you have daily weather data, the context length would be 12 time steps.\n",
    "\n",
    "The specific context length can be fine-tuned based on the granularity of your weather data and computational constraints. But in general, a 12-day or longer context should allow the model to learn meaningful relationships between the SAR observations and meteorological conditions for flood forecasting in Malaysia.\n"
   ],
   "id": "c7316368ba4c8d1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flood Data Collection and Processing\n",
    "\n",
    "This code is designed to collect and process flood-related data from various sources and create balanced train and validation datasets for flood prediction.\n",
    "\n",
    "The `FloodDataCollector` class takes a GeoJSON file or dictionary, start and end dates, a time resolution, and an output directory as input. It then performs the following steps:\n",
    "\n",
    "1. Loads the GeoJSON data and extracts the bounding box coordinates.\n",
    "2. Retrieves Sentinel-1 satellite data for the specified bounding box and time range.\n",
    "3. Retrieves Copernicus DEM (Digital Elevation Model) data for the specified bounding box.\n",
    "4. Retrieves meteorological data from the Met Malaysia API for the specified location and time range.\n",
    "5. Retrieves flood data from the Open-Meteo API for the specified location and time range.\n",
    "6. Combines the collected data into a single dataset.\n",
    "7. Balances the dataset by oversampling the minority class (flood instances) to ensure an equal number of flood and no-flood instances.\n",
    "8. Splits the balanced dataset into train and validation sets with an 80/20 split.\n",
    "9. Saves the train and validation datasets as `train_dataset.pt` and `val_dataset.pt` files in the specified output directory.\n",
    "\n",
    "The code utilizes caching mechanisms to avoid redundant API calls and improve performance. It also applies normalization to the collected data to ensure consistent scaling.\n",
    "\n",
    "The resulting train and validation datasets can be used for training and evaluating flood prediction models.\n"
   ],
   "id": "6d5b8f1530052b50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T09:46:03.303583Z",
     "start_time": "2024-05-15T09:46:02.287929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "# Assuming the required inputs are loaded in previous cells\n",
    "geojson_path = \"path/to/your/geojson/file.geojson\"\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = datetime(2024, 1, 31)\n",
    "resolution = timedelta(hours=1)\n",
    "output_dir = \"output\"\n",
    "\n",
    "# Create an instance of the FloodDataCollector\n",
    "collector = FloodDataCollector(geojson_path, start_date, end_date, resolution, output_dir)\n",
    "\n",
    "# Collect and process the data\n",
    "collector.collect_data()\n",
    "\n",
    "# Load the generated train and validation datasets\n",
    "train_dataset = torch.load(os.path.join(output_dir, \"train_dataset.pt\"))\n",
    "val_dataset = torch.load(os.path.join(output_dir, \"val_dataset.pt\"))"
   ],
   "id": "b3fc005912cf364d",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Assuming the required inputs are loaded in previous cells\u001B[39;00m\n\u001B[1;32m      5\u001B[0m geojson_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath/to/your/geojson/file.geojson\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m start_date \u001B[38;5;241m=\u001B[39m \u001B[43mdatetime\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2010\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m end_date \u001B[38;5;241m=\u001B[39m datetime(\u001B[38;5;241m2024\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m31\u001B[39m)\n\u001B[1;32m      8\u001B[0m resolution \u001B[38;5;241m=\u001B[39m timedelta(hours\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'module' object is not callable"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the training cell, we will initialize the `ModelTrainer` class with the provided `FloodForecastingDataLoader` instance. This class is responsible for loading the data and preparing it for training and validation. The `ModelTrainer` class also includes methods for loading a pre-trained model, training the model with the specified parameters (number of epochs, batch size, and learning rate), and saving the trained model.\n",
    "\n",
    "The training process includes the following steps:\n",
    "- Preparing the training and validation datasets.\n",
    "- Loading a pre-trained `PatchTSMixerForPrediction` model.\n",
    "- Setting up the training arguments, including the number of epochs, batch size, learning rate, and logging directory.\n",
    "- Initializing the `Trainer` with the model, training arguments, and datasets.\n",
    "- Executing the training loop using the `trainer.train()` method.\n",
    "\n",
    "After training, the model is evaluated on the validation dataset to calculate the F1-score, and the results are logged. The trained model is then saved to the specified directory."
   ],
   "id": "53789d9d62979941"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T09:02:50.191187Z",
     "start_time": "2024-05-15T09:02:50.182797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.dataloader import FloodForecastingDataLoader\n",
    "from model.modeltrainer import ModelTrainer\n",
    "# Initialize the data loader with the required parameters\n",
    "train_data_loader = FloodForecastingDataLoader(train_dataset, forecast_horizon=96) # last 48 hrs iignored for training \n",
    "\n",
    "# Create an instance of the ModelTrainer class\n",
    "trainer_instance = ModelTrainer(train_data_loader)\n",
    "\n",
    "# Train the model with the specified parameters\n",
    "trainer_instance.train()\n",
    "\n",
    "\n"
   ],
   "id": "724fd79ba30149c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the validation cell, we will utilize the `validate_and_evaluate` method of the `ModelTrainer` class to assess the performance of the trained model on the validation dataset. This method performs the following steps:\n",
    "- It loads the validation dataset into a `DataLoader` with the appropriate batch size.\n",
    "- It uses the `Trainer` to predict the outcomes on the validation dataset.\n",
    "- It post-processes the predictions to convert them into binary flood predictions.\n",
    "- It calculates the F1-score, which is a harmonic mean of precision and recall, and is a measure of the model's accuracy on the binary classification task.\n",
    "- It generates and saves the ROC curve and confusion matrix plots to visualize the model's performance.\n",
    "\n",
    "The outputs of this process include:\n",
    "- F1-score: A single value indicating the model's accuracy.\n",
    "- ROC curve: A plot showing the relationship between the true positive rate and false positive rate at various threshold settings.\n",
    "- Confusion matrix: A heatmap showing the true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\n"
   ],
   "id": "ca126cae9767a897"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T09:30:11.474133Z",
     "start_time": "2024-05-15T09:30:11.464107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model.modeltrainer import ModelTrainer\n",
    "\n",
    "\n",
    "# Initialize the data loader with the required parameters\n",
    "validate = ModelTrainer(FloodForecastingDataLoader(val_dataset, forecast_horizon=96))\n",
    "# Assuming the ModelTrainer instance 'trainer_instance' has already been created and trained in previous cells\n",
    "\n",
    "# Prepare the validation dataset\n",
    "_, val_dataset = trainer_instance.prepare_data()\n",
    "\n",
    "# Load the trained model (this assumes the model has been trained and saved in the previous steps)\n",
    "model = trainer_instance.load_pretrained_model()\n",
    "\n",
    "# Validate and evaluate the model, which will also generate the ROC curve and confusion matrix plots\n",
    "f1_score = trainer_instance.validate_and_evaluate(val_dataset, model)\n",
    "\n",
    "# Log the results\n",
    "print(f\"Validation F1 Score: {f1_score}\")"
   ],
   "id": "9cde6a965bbcc826",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Validation Loop Code Cell\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Assuming the ModelTrainer instance 'trainer_instance' has already been created and trained in previous cells\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Prepare the validation dataset\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m _, val_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer_instance\u001B[49m\u001B[38;5;241m.\u001B[39mprepare_data()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Load the trained model (this assumes the model has been trained and saved in the previous steps)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m trainer_instance\u001B[38;5;241m.\u001B[39mload_pretrained_model()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trainer_instance' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visual Training Dashboard\n",
    "\n",
    "Here's how the script works:\n",
    "\n",
    "The script creates a Dash app with a layout that includes graphs for validation loss, ROC curve, and confusion matrix.\n",
    "The ModelTrainer class is modified to update the global variables val_loss_data, roc_curve_data, and confusion_matrix_data during the training process.\n",
    "The run_dashboard function is defined to set up the data loader, create an instance of ModelTrainer, and start the model training in a separate thread.\n",
    "The Dash app is run using app.run_server(), which starts the dashboard and allows it to update in real-time.\n",
    "The callbacks for updating the graphs are defined using the @app.callback decorator. These callbacks are triggered at regular intervals (every 5 seconds in this example) to update the graphs with the latest data.\n",
    "\n",
    "To run the script from a Jupyter notebook, you can use the %run magic command followed by the path to the script file. For example:\n",
    "This will execute the script and start the Dash app, which you can then access through the provided URL in the notebook output.\n",
    "Note: Make sure to set the appropriate values for geojson_path, start_date, end_date, forecast_horizon, epochs, batch_size, and learning_rate before running the script."
   ],
   "id": "fbdbee8a1c7bb825"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run path/to/dashboard_script.py",
   "id": "bbc2a7fddf3954aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9aa23d9b883539f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
